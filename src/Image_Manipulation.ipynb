{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Video Anomaly Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Number of GPUs Available: \", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from utils import *\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from numba import jit, cuda\n",
    "\n",
    "from PIL import Image, ImageChops, ImageEnhance,ImageFilter, ImageOps\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "from scipy.fftpack import dct\n",
    "from scipy import ndimage\n",
    "from scipy import fftpack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train, Y_train, X_val, Y_val, img_size=(128,128)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(img_size[0], img_size[1], 3)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    epochs = 30\n",
    "    batch_size = 32\n",
    "    init_lr = 1e-4\n",
    "    opt = Adam(lr=init_lr, decay=init_lr / epochs)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "    checkpoint = ModelCheckpoint('../models/model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, Y_val), callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "    # Plot training and validation accuracy and loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    #guardamos la gráfica , ponemos el nombre del modelo y la precisión\n",
    "    plt.savefig('../graphs/model_acc_'+str(history.history['val_accuracy'][-1])+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    #guardamos la gráfica , ponemos el nombre del modelo y la precisión\n",
    "    plt.savefig('../graphs/model_loss_'+str(history.history['val_accuracy'][-1])+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    #guardamos el modelo\n",
    "    model.save('../models/model'+str(history.history['val_accuracy'][-1])+'.h5')\n",
    "\n",
    "    score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model_2(X_train, Y_train, X_val, Y_val, img_size=(128,128)):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(img_size[0], img_size[1], 3)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "    init_lr = 1e-4\n",
    "    opt = Adam(lr=init_lr, decay=init_lr / epochs)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "    checkpoint = ModelCheckpoint('../models/model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "\n",
    "    # Plot training and validation accuracy and loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    #guardamos la gráfica , ponemos el nombre del modelo y la precisión\n",
    "    plt.savefig('../graphs/model_acc_'+str(history.history['val_accuracy'][-1])+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    #guardamos la gráfica , ponemos el nombre del modelo y la precisión\n",
    "    plt.savefig('../graphs/model_loss_'+str(history.history['val_accuracy'][-1])+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    #guardamos el modelo\n",
    "    model.save('../models/model'+str(history.history['val_accuracy'][-1])+'.h5')\n",
    "\n",
    "    score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_3(X_train, Y_train, X_val, Y_val, img_size=(128,128)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (img_size[0], img_size[1], 3)))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (img_size[0], img_size[1], 3)))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "    init_lr = 1e-4\n",
    "    opt = Adam(lr=init_lr, decay=init_lr / epochs)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "    checkpoint = ModelCheckpoint('../models/model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, checkpoint])\n",
    "    \n",
    "    # Plot training and validation accuracy and loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    #guardamos la gráfica , ponemos el nombre del modelo y la precisión\n",
    "    plt.savefig('../graphs/model_acc_'+str(history.history['val_accuracy'][-1])+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    #guardamos la gráfica , ponemos el nombre del modelo y la precisión\n",
    "    plt.savefig('../graphs/model_loss_'+str(history.history['val_accuracy'][-1])+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    #guardamos el modelo\n",
    "    model.save('../models/model'+str(history.history['val_accuracy'][-1])+'.h5')\n",
    "\n",
    "    score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_splice_classification_model(X_train, Y_train, X_val, Y_val):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    epochs = 40\n",
    "    batch_size = 32\n",
    "    init_lr = 1e-4\n",
    "    opt = Adam(lr=init_lr, decay=init_lr / epochs)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 2,verbose = 0,mode = 'auto')\n",
    "    \n",
    "    model.fit(X_train,Y_train,batch_size = batch_size, epochs = epochs,validation_data = (X_val, Y_val), callbacks = [early_stopping])\n",
    "\n",
    "    #calculamos la precisión del modelo\n",
    "    score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    model.save('models/splice_model'+str(score[1])+'.h5')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    real_images_path = '../data/dataset/data/CASIA2/Au/*.*'\n",
    "    fake_images_path = '../data/dataset/data/CASIA2/Tp/*.*'\n",
    "    image_size = (128, 128)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    print(\"Cargando datos de imágenes reales...\")\n",
    "    for file_path in glob.glob(real_images_path):\n",
    "            X.append(preparete_image_ela(file_path, image_size))\n",
    "            y.append(0)\n",
    "\n",
    "    # poner de manera aleatoria las imágenes \n",
    "    random.shuffle(X)\n",
    "   \n",
    "    print(\"Cargando datos de imágenes falsificadas...\")\n",
    "    for file_path in glob.glob(fake_images_path):\n",
    "            X.append(preparete_image_ela(file_path, image_size))\n",
    "            y.append(1)\n",
    "\n",
    "\n",
    "    print(\"X shape: \", np.array(X).shape)\n",
    "    print(\"y shape: \", np.array(y).shape)\n",
    "\n",
    "    X= np.array(X).reshape(-1, image_size[0], image_size[1], 3)\n",
    "    y = to_categorical(y, num_classes = 2)\n",
    "\n",
    "    # Entrenamos con 80% de los datos de train, usar 10% para validación y 10% para test\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size = 0.5, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train, Y_train, X_val, Y_val,image_size)\n",
    "#model = tf.keras.models.load_model('../models/MODEL_ELA_V4_094_TEST.h5')\n",
    "\n",
    "# Calculamos la precisión del modelo con los datos de test\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Calcula la matriz de confusión\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_true=Y_test.argmax(axis=1), y_pred=y_pred.argmax(axis=1))\n",
    "\n",
    "# Libera la memoria de la GPU\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../models/MODEL_ELA_V4_0.90.h5')\n",
    "\n",
    "# Calculamos la precisión del modelo con los datos de test\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "class_names = ['real', 'fake']\n",
    "\n",
    "image_size = (128, 128)\n",
    "\n",
    "#Obtener todos los path de las imagenes de test\n",
    "test_images_path = '../data/dataset/data/CASIA2/Tp/*.*'\n",
    "\n",
    "correcto= 0\n",
    "total = 0\n",
    "\n",
    "for file_path in glob.glob(test_images_path):\n",
    "    img = preparete_image_ela(file_path, image_size)\n",
    "    img = np.array(img).reshape(-1, image_size[0], image_size[1], 3)\n",
    "    y_pred = model.predict(img)\n",
    "    y_pred_class = np.argmax(y_pred, axis = 1)[0]\n",
    "    total += 1\n",
    "    if y_pred_class == 1:\n",
    "        correcto += 1\n",
    "        print(f'Class: {class_names[1]} Confidence: {np.amax(y_pred) * 100:0.2f}')\n",
    "    \n",
    "print(f'Total: {total}, Correct: {correcto}, Acc: {correcto / total * 100.0}')\n",
    "    \n",
    "\n",
    "# mostrar imagen y escribir el resultado en la imagen\n",
    "img = cv2.imread(file_path)\n",
    "cv2.putText(img, f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "# show with matplotlib\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
